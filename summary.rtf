{\rtf1\ansi\ansicpg1252\cocoartf1343\cocoasubrtf140
{\fonttbl\f0\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sa240

\f0\fs32 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Team Name:- Quizzly Bears\
Team Members: Soumya Smruti Mishra, Neetu Pathak\
1. System Components and Team Member Contributions:- There were two files for this system, the main file being quesAnswered.py and the other phrase_process.py.\
Most of the source code for the system lies in quesAnswered.py. It contains the data extraction code, then the sentence scoring code and then answer sentence detection code. We used python dictionaries to store the sentence and parsed pos and ner tags in the dictionary of stories. The question dictionary contained quest and parsed POS and NER tags. Since we implemented the rule based question answering system the rules for different question types also was there in this file. We return the best sentence after scoring it and remove question words from it. From the best sentence we returned words and phrases depending on the question type as well.\
We returned phrases for some of the Where questions from the parameter of the sentence so the code for that lies in the phrase_process.py.\
I, Soumya was responsible for most of the coding sections and Neetu was responsible for debugging and analysis of the system and generating new rules for some questions.\
2. External Resources:- We used Stanford Core NLP python wrapper for POS Tagging, NER, Co-reference Resolution and building parse tree. https://github.com/dasmith/stanford-corenlp-python was the wrapper we used.\
We used WordNet lemmatizer for lemmatization purpose. \
We also used WordNet for tagging person words if any missed by the NER tagging.\
Both of these were available in NLTK.\
3. Regrets:- Co-reference resolution didn\'92t increase our score greatly as we spent a lot of time coding it and also WordNet was also not that successful. We should have spend more time on analyzing question by question and improving our precision because our recall was pretty high (62) as compared to other top teams.\
4. Successes:- Writing the code as a whole for Rule based system worked very well. Removing question words from sentence improved our precision greatly. Some new rules that we wrote for \'93what are\'94 type questions worked very well. \
\
\
}